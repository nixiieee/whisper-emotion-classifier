# whisper-emotion-classifier

**Emotion classification from speech using Whisper encoder + MLP head**  
The goal of the project is to fine-tune the model based on encoder of OpenAI's Whisper to classify emotions in Russian speech and to evaluate its quality using Monte Carlo methods.

---

## üìÅ Project Structure

```
whisper-emotion-classifier/
‚îÇ
‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îú‚îÄ‚îÄ gradio_pipeline.py     # Gradio demo pipeline
‚îÇ   ‚îî‚îÄ‚îÄ inference.py           # Inference classes and logic
‚îÇ
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îî‚îÄ‚îÄ train_small.py         # Whisper-small training script
‚îÇ
‚îú‚îÄ‚îÄ preprocess_data.ipynb      # Data preprocessing pipeline
‚îú‚îÄ‚îÄ MC_dropout.ipynb           # MC Dropout experiments for uncertainty
‚îú‚îÄ‚îÄ bootstrap.ipynb            # Bootstrap analysis for uncertainty
‚îú‚îÄ‚îÄ requirements.txt           # List of dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## ü§ó Hugging Face Resources

- Dataset: [`nixiieee/dusha_balanced`](https://huggingface.co/datasets/nixiieee/dusha_balanced)  
- Model: [`nixiieee/whisper-small-emotion-classifier-dusha`](https://huggingface.co/nixiieee/whisper-small-emotion-classifier-dusha)

---

## ‚öôÔ∏è Model Architecture

The model consists of:

- **Encoder**: Pretrained Whisper encoder (`openai/whisper-small`)
- **Head**: Lightweight MLP for emotion classification (softmax output)
- Fine-tuned on emotional Russian speech (multi-class classification)

---

## üöÄ Quickstart

### 1. Install dependencies

#### Using [`uv`](https://github.com/astral-sh/uv) (recommended):

```bash
uv venv --python 3.11
source .venv/bin/activate  # or .venv\Scripts\activate on Windows
uv pip install -r requirements.txt
```

#### Alternatively, using pip:

```bash
pip install -r requirements.txt
```

### 2. Launch Gradio App

```bash
python3 inference/gradio_pipeline.py
```

This will open a local Gradio interface for real-time emotion recognition from audio or video (`.mp3`, `.mp4` and `.wav` formats are supported).

---

## üìä Logging & Tracking with Weights & Biases (W&B)

During training, the model logs metrics and visualizations to [Weights & Biases](https://wandb.ai/):

- Training & validation loss
- Accuracy per epoch
- Confusion matrix & misclassification analysis

To enable logging, make sure you are logged in:

```bash
wandb login
```

By default, W&B is integrated in the training script:

```python
import wandb
wandb.init(project="whisper-emotion")
```

You can monitor training in real time at [wandb.ai](https://wandb.ai/) or in your terminal.

---

## üß™ Experiments & Analysis

We performed uncertainty estimation using:

- `MC_dropout.ipynb`: Monte Carlo Dropout for evaluating prediction confidence
- `bootstrap.ipynb`: Bootstrap analysis for statistical variability
- `preprocess_data.ipynb`: Preprocessing steps ‚Äî label mapping, splitting, etc.

---

## üì´ Contact

For questions or collaboration ideas, feel free to reach out via GitHub Issues.

---

> Made with ‚ù§Ô∏è for movie lovers and ML enthusiasts
